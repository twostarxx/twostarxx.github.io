<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[Cost Function的原理及实现(Python, Matlab)]]></title>
      <url>/2017/11/18/Cost-Function%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0-Python-Matlab/</url>
      <content type="html"><![CDATA[<h2 id="成本函数-Cost-Function"><a href="#成本函数-Cost-Function" class="headerlink" title="成本函数(Cost Function)"></a>成本函数(Cost Function)</h2><p>$$J(\theta_0,\theta<em>1) = \frac{1}{2m}\sum</em>{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2$$<br>$m$ :  Number of training examples.<br>$x$ : input.<br>$y$ : output.<br>Parameters: $\theta_0$, $\theta_1$.</p>
<p>以下为MATLAB实现方式：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">J</span> = <span class="title">computeCost</span><span class="params">(X, y, theta)</span></span></div><div class="line">	<span class="comment">%COMPUTECOST Compute cost for linear regression</span></div><div class="line">	<span class="comment">%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the parameter for linear regression to fit the data points in X and y</span></div><div class="line">	m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></div><div class="line">	J = <span class="number">0</span>;</div><div class="line">	J = sum((X * theta - y) .^<span class="number">2</span>) / <span class="number">2</span> / m;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>以下为Python实现方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(X, y, theta)</span>:</span></div><div class="line">    <span class="string">"""Cost Functions</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Parameters</span></div><div class="line"><span class="string">    ----------</span></div><div class="line"><span class="string">    X : np.ndarray, like (49 * 2)</span></div><div class="line"><span class="string">    y : np.ndarray, like (49 * 1)</span></div><div class="line"><span class="string">    theta : np.ndarray, like (2 * 1)</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns</span></div><div class="line"><span class="string">    -------</span></div><div class="line"><span class="string">    J : float, cost</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    """</span></div><div class="line">    y = np.transpose(y)</div><div class="line">    J = sum((np.dot(X, theta) - y.reshape(len(y),<span class="number">1</span>)) **<span class="number">2</span>) / <span class="number">2.0</span> / len(y)</div><div class="line">    <span class="comment"># np.dot(A, B) 矩阵乘积，A * B 矩阵点乘</span></div><div class="line">    <span class="comment"># pow(A, 2) 多次乘积，**n 点次方</span></div><div class="line">    <span class="keyword">return</span> J</div></pre></td></tr></table></figure>
<p>注意：np中点乘和矩阵乘积，点次方与多次乘积的区别，与MATLAB不同。</p>
<h2 id="梯度下降法-Gradient-Descent"><a href="#梯度下降法-Gradient-Descent" class="headerlink" title="梯度下降法(Gradient Descent)"></a>梯度下降法(Gradient Descent)</h2><p>$$repeat\quad until\quad convergence { \quad<br>\theta_j:=\theta_j-\alpha \frac{\partial J(\theta_0, \theta_1)}{\partial \theta_j} \quad (j=0,1) \quad }$$<br>$\alpha$ : learning rate.<br>$\partial$ : 偏微分</p>
<p>以下为MATLAB实现：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta, J_history]</span> = <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, num_iters)</span></span></div><div class="line">	<span class="comment">%GRADIENTDESCENT Performs gradient descent to learn theta</span></div><div class="line">	<span class="comment">%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by </span></div><div class="line">	<span class="comment">%   taking num_iters gradient steps with learning rate alpha</span></div><div class="line">	</div><div class="line">	m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></div><div class="line">	J_history = <span class="built_in">zeros</span>(num_iters, <span class="number">1</span>);</div><div class="line">	</div><div class="line">	<span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</div><div class="line">	    theta = theta -  X' * (X * theta - y) * (alpha / m);  </div><div class="line">	    J_history(iter) = computeCost(X, y, theta);</div><div class="line">	<span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>以下为Python实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, num_iters)</span>:</span></div><div class="line">    <span class="string">"""Gradient Descent for (Multivariate) Linear Regression</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Parameters</span></div><div class="line"><span class="string">    ----------</span></div><div class="line"><span class="string">    X : np.ndarray, like (49 * 2)</span></div><div class="line"><span class="string">    y : np.ndarray, like (49 * 1)</span></div><div class="line"><span class="string">    theta : np.ndarray, like (49 * 1)</span></div><div class="line"><span class="string">    alpha : learning rate</span></div><div class="line"><span class="string">    num_iters : number of iter</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    Returns</span></div><div class="line"><span class="string">    -------</span></div><div class="line"><span class="string">    tuple(J_history, theta)</span></div><div class="line"><span class="string">    J_history : np.ndarray, like (num_iters, 1)</span></div><div class="line"><span class="string">    theta : theta of convergence, like (2 * 1)</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    J_history = np.zeros((num_iters, <span class="number">1</span>))</div><div class="line">    <span class="keyword">for</span> n_iter <span class="keyword">in</span> range(num_iters):</div><div class="line">        theta = theta - np.dot(X.T, np.dot(X, theta) - y.reshape(len(y),<span class="number">1</span>)) *alpha / len(y)</div><div class="line">        J_history[n_iter, <span class="number">0</span>] = computeCost(X, y, theta)</div><div class="line">    <span class="keyword">return</span> J_history, theta</div></pre></td></tr></table></figure>
<h2 id="线性回归中的梯度下降公式"><a href="#线性回归中的梯度下降公式" class="headerlink" title="线性回归中的梯度下降公式"></a>线性回归中的梯度下降公式</h2><p>$$\theta_0:= \theta<em>0-\alpha\frac{1}{m} \sum</em>{i=1}^m(h_\theta(x^{(i)})-y^{(i)})$$<br>$$\theta_1:= \theta<em>1-\alpha\frac{1}{m}\sum</em>{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_1^{(i)}$$<br>同理可得：<br>$$\theta_j:= \theta<em>j-\alpha\frac{1}{m}\sum</em>{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$<br>在（多元）线性回归中，由于在代码中直接使用矩阵进行运算，因此代码同上。</p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[Collections.sort(List<T> list, Comparator<? super T> c) 源代码分析]]></title>
      <url>/2017/10/24/Collections-sort-List-T-list-Comparator-super-T-c-%E6%BA%90%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<p>Collections.sort(List<t> list, Comparator? super T c)<br>Sorts the specified list according to the order induced by the specified comparator.</t></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function"><span class="keyword">void</span> <span class="title">sort</span><span class="params">(List&lt;T&gt; list, Comparator&lt;? <span class="keyword">super</span> T&gt; c)</span> </span>&#123;</div><div class="line">    Object[] a = list.toArray();  </div><div class="line">    Arrays.sort(a, (Comparator)c);  </div><div class="line">    ListIterator i = list.listIterator();  </div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;a.length; j++) &#123;  </div><div class="line">        i.next();  </div><div class="line">        i.set(a[j]);  </div><div class="line">    &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>该方法主要细分为三个步骤：</p>
<ol>
<li>将list装换成一个对象数组。</li>
<li>将这个对象数组传递给Arrays类的sort方法（也就是说collections的sort其实本质是调用了Arrays.sort）。</li>
<li>完成排序之后，再一个一个地，把Arrays的元素复制到List中。</li>
</ol>
<p>具体例子见LintCode156题合并区间，修改实例如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 先对区间进行排序，使用一个匿名内部类</span></div><div class="line">Collections.sort(intervals, <span class="keyword">new</span> Comparator&lt;Interval&gt;() &#123;</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Interval o1, Interval o2)</span> </span>&#123;</div><div class="line">        <span class="keyword">return</span> o1.start - o2.start;</div><div class="line">    &#125;</div><div class="line">&#125;);</div></pre></td></tr></table></figure></p>
]]></content>
      
        
        <tags>
            
            <tag> Java </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[博客搬运中]]></title>
      <url>/2017/10/24/%E5%8D%9A%E5%AE%A2%E6%90%AC%E8%BF%90%E4%B8%AD/</url>
      <content type="html"><![CDATA[<h2 id="博客搬运"><a href="#博客搬运" class="headerlink" title="博客搬运"></a>博客搬运</h2><p>包括：CSDN，sina等。</p>
<h2 id="Coming-soom…"><a href="#Coming-soom…" class="headerlink" title="Coming soom…"></a>Coming soom…</h2>]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[Android Studio创建虚拟器错误]]></title>
      <url>/2017/10/24/Android-Studio%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E5%99%A8%E9%94%99%E8%AF%AF/</url>
      <content type="html"><![CDATA[<p>在Android Studio的使用过程中，发现没有办法创建或运行虚拟机。在此记录一波三折的解决过程。<br><strong>最终解决方法：使用Genymotion模拟器插件代替原有虚拟器。</strong></p>
<h2 id="1-Error-Your-CPU-does-not-support-required-features-VT-x-or-SVM"><a href="#1-Error-Your-CPU-does-not-support-required-features-VT-x-or-SVM" class="headerlink" title="1.Error: Your CPU does not support required features (VT-x or SVM)."></a>1.Error: Your CPU does not support required features (VT-x or SVM).</h2><p>使用Android Studio自带模拟器进行创建时，出现bug见下图。<br> <img src="http://oybyil7f9.bkt.clouddn.com/AS.JPG" alt="图1"></p>
<p>查到前辈rznice的博客：<br><a href="http://blog.csdn.net/rznice/article/details/40210213" target="_blank" rel="external">http://blog.csdn.net/rznice/article/details/40210213</a><br>根据步骤安装 intelhaxm.exe，再次报错见点二。</p>
<h2 id="2-Error-This-computer-does-not-support-VT-x"><a href="#2-Error-This-computer-does-not-support-VT-x" class="headerlink" title="2.Error:This computer does not support VT-x."></a>2.Error:This computer does not support VT-x.</h2><p>安装intelhaxm.exe时，出现下图所示。<br><img src="http://oybyil7f9.bkt.clouddn.com/AS2.JPG" alt="图2"><br>一度怀疑自己没有在BIOS中打开Virtualization Technology，多次确认打开后，重新安装intelhaxm.exe，依旧没有任何用处。<br>找到其他人的回答：<br><a href="http://stackoverflow.com/questions/16091677/intel-haxm-installation-error-this-computer-does-not-support-intel-virtualizat/18796753" target="_blank" rel="external">http://stackoverflow.com/questions/16091677/intel-haxm-installation-error-this-computer-does-not-support-intel-virtualizat/18796753</a><br>发现自己的电脑中并<strong>没有hyper-V</strong>，特意查看了别人的电脑，有的有，有的没。根据教程下载Intel Processor Identification Utility，试图安装hyper-V，安装失败。失败的原因见点3。<br>ps：发现一位大神翻遍了微软的官方文档，出个镜：<br><a href="http://www.crifan.com/adt_haxm_xd_not_supported_this_computer_does_not_support_intel_execute_disable_bit_xd_or_it_is_disable_in_the_bios/" target="_blank" rel="external">http://www.crifan.com/adt_haxm_xd_not_supported_this_computer_does_not_support_intel_execute_disable_bit_xd_or_it_is_disable_in_the_bios/</a></p>
<h2 id="3-AMD处理器不能安装HAXM"><a href="#3-AMD处理器不能安装HAXM" class="headerlink" title="3.AMD处理器不能安装HAXM"></a>3.AMD处理器不能安装HAXM</h2><p>当我发现这个问题的时候，我开始怀疑无解了，然后开始想其他的办法。下图为我自己的处理器。<br><img src="http://oybyil7f9.bkt.clouddn.com/AS3.JPG" alt="图3"><br><strong>解决方式：Genymotion+AS插件</strong></p>
<h2 id="4-安装Genymotion"><a href="#4-安装Genymotion" class="headerlink" title="4.安装Genymotion"></a>4.安装Genymotion</h2><p>点击官网：<a href="http://www.genymotion.net/" target="_blank" rel="external">http://www.genymotion.net/</a>.<br>Genymotion有免费版和收费版，个人用免费版就够了。<br><img src="http://oybyil7f9.bkt.clouddn.com/AS4.JPG" alt="图4"><br>选择V-box，点击开始下载，安装过程略。<br><img src="http://oybyil7f9.bkt.clouddn.com/AS5.JPG" alt="图5"><br>备注：必须注册后才能下载，请记牢密码，下载安装时需要登录。<br><img src="http://oybyil7f9.bkt.clouddn.com/AS6.JPG" alt="图6"></p>
<h2 id="5-AS插件"><a href="#5-AS插件" class="headerlink" title="5.AS插件"></a>5.AS插件</h2><p>网上教程比较多，就简单写写。<br>Android Studio –&gt; File –&gt; setting –&gt; plugins –&gt; Brower –&gt; 输入Genymotion –&gt; Download –&gt; restart AS.<br>出现下图最右边的图标则表示安装成功。<br><img src="http://oybyil7f9.bkt.clouddn.com/AS7.JPG" alt="图7"><br>第一次运行Genymotion需要配置路径，就不赘述了。</p>
<p>附上成功界面。<br><img src="http://oybyil7f9.bkt.clouddn.com/AS8.JPG" alt="图8"></p>
]]></content>
      
        
        <tags>
            
            <tag> Android </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>/2017/10/24/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
      
        
    </entry>
    
  
  
</search>
